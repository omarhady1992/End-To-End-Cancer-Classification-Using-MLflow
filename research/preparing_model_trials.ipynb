{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for the preliminary trials of creating a basic classification model using tensor flow\n",
    "the model will be a transfer learning model for a cnn network based on vgg 16 model found in the tensorflow keras models library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow==2.2.2 (from -r required_packages.txt (line 1))\n",
      "  Using cached mlflow-2.2.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyYAML (from -r required_packages.txt (line 2))\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting types-pyYAML (from -r required_packages.txt (line 3))\n",
      "  Using cached types_PyYAML-6.0.12.20240917-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting scipy (from -r required_packages.txt (line 4))\n",
      "  Downloading scipy-1.14.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting seaborn (from -r required_packages.txt (line 5))\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from -r required_packages.txt (line 6)) (2.0.2)\n",
      "Collecting pandas (from -r required_packages.txt (line 7))\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting gdown (from -r required_packages.txt (line 8))\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting notebook (from -r required_packages.txt (line 9))\n",
      "  Downloading notebook-7.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting dvc (from -r required_packages.txt (line 10))\n",
      "  Using cached dvc-3.56.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tqdm (from -r required_packages.txt (line 11))\n",
      "  Using cached tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting matplotlib (from -r required_packages.txt (line 12))\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting joblib (from -r required_packages.txt (line 13))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting ensure==1.0.2 (from -r required_packages.txt (line 14))\n",
      "  Using cached ensure-1.0.2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting python-box==6.0.2 (from -r required_packages.txt (line 15))\n",
      "  Using cached python_box-6.0.2-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting Flask (from -r required_packages.txt (line 16))\n",
      "  Downloading flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting Flask-cors (from -r required_packages.txt (line 17))\n",
      "  Using cached Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting click<9,>=7.0 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cloudpickle<3 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting databricks-cli<1,>=0.8.7 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached databricks_cli-0.18.0-py2.py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting entrypoints<1 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting gitpython<4,>=2.1.0 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting protobuf<5,>=3.12.0 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting pytz<2023 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached pytz-2022.7.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from mlflow==2.2.2->-r required_packages.txt (line 1)) (2.32.3)\n",
      "Collecting packaging<24 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting importlib-metadata!=4.7.0,<7,>=3.7.0 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading sqlparse-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting alembic<2 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting docker<7,>=4.0.0 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached docker-6.1.3-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting Flask (from -r required_packages.txt (line 16))\n",
      "  Using cached flask-2.3.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting numpy (from -r required_packages.txt (line 6))\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting querystring-parser<2 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading SQLAlchemy-2.0.36-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting scikit-learn<2 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting pyarrow<12,>=4.0.0 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached pyarrow-11.0.0-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting shap<1,>=0.40 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached shap-0.46.0-cp310-cp310-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from mlflow==2.2.2->-r required_packages.txt (line 1)) (3.7)\n",
      "Collecting waitress<3 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached waitress-2.1.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting Jinja2<4,>=3.0 (from mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from ensure==1.0.2->-r required_packages.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from pandas->-r required_packages.txt (line 7)) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7 (from pandas->-r required_packages.txt (line 7))\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting beautifulsoup4 (from gdown->-r required_packages.txt (line 8))\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from gdown->-r required_packages.txt (line 8))\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from notebook->-r required_packages.txt (line 9))\n",
      "  Downloading jupyter_server-2.14.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from notebook->-r required_packages.txt (line 9))\n",
      "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jupyterlab<4.3,>=4.2.0 (from notebook->-r required_packages.txt (line 9))\n",
      "  Downloading jupyterlab-4.2.6-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting notebook-shim<0.3,>=0.2 (from notebook->-r required_packages.txt (line 9))\n",
      "  Downloading notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from notebook->-r required_packages.txt (line 9)) (6.4.1)\n",
      "Collecting attrs>=22.2.0 (from dvc->-r required_packages.txt (line 10))\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting celery (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached celery-5.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: colorama>=0.3.9 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from dvc->-r required_packages.txt (line 10)) (0.4.6)\n",
      "Collecting configobj>=5.0.6 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached configobj-5.0.9-py2.py3-none-any.whl\n",
      "Collecting distro>=1.3 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting dpath<3,>=2.1.0 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached dpath-2.2.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting dulwich (from dvc->-r required_packages.txt (line 10))\n",
      "  Downloading dulwich-0.22.6-cp310-cp310-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting dvc-data<3.17,>=3.16.2 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached dvc_data-3.16.7-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting dvc-http>=2.29.0 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached dvc_http-2.32.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting dvc-objects (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached dvc_objects-5.1.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting dvc-render<2,>=1.0.1 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached dvc_render-1.0.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dvc-studio-client<1,>=0.21 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached dvc_studio_client-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting dvc-task<1,>=0.3.0 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached dvc_task-0.40.2-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting flatten-dict<1,>=0.4.1 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached flatten_dict-0.4.2-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting flufl.lock<9,>=8.1.0 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached flufl_lock-8.1.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec>=2024.2.0 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting funcy>=1.14 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting grandalf<1,>=0.7 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting gto<2,>=1.6.0 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached gto-1.7.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting hydra-core>=1.1 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting iterative-telemetry>=0.0.7 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached iterative_telemetry-0.0.9-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting kombu (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached kombu-5.4.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting networkx>=2.5 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting omegaconf (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pathspec>=0.10.3 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting platformdirs<4,>=3.1.1 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached platformdirs-3.11.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: psutil>=5.8 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from dvc->-r required_packages.txt (line 10)) (6.1.0)\n",
      "Collecting pydot>=1.2.4 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached pydot-3.0.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pygtrie>=2.3.2 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pyparsing>=2.4.7 (from dvc->-r required_packages.txt (line 10))\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: rich>=12 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from dvc->-r required_packages.txt (line 10)) (13.9.4)\n",
      "Collecting ruamel.yaml>=0.17.11 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting scmrepo<4,>=3.3.8 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached scmrepo-3.3.8-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting shortuuid>=0.5 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting shtab<2,>=1.3.4 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tabulate>=0.8.7 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting tomlkit>=0.11.1 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting voluptuous>=0.11.7 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached voluptuous-0.15.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting zc.lockfile>=1.2.1 (from dvc->-r required_packages.txt (line 10))\n",
      "  Using cached zc.lockfile-3.0.post1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r required_packages.txt (line 12))\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r required_packages.txt (line 12))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r required_packages.txt (line 12))\n",
      "  Downloading fonttools-4.55.0-cp310-cp310-win_amd64.whl.metadata (167 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r required_packages.txt (line 12))\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting pillow>=8 (from matplotlib->-r required_packages.txt (line 12))\n",
      "  Downloading pillow-11.0.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: Werkzeug>=2.3.7 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from Flask->-r required_packages.txt (line 16)) (3.1.3)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask->-r required_packages.txt (line 16))\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting blinker>=1.6.2 (from Flask->-r required_packages.txt (line 16))\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting Mako (from alembic<2->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from alembic<2->mlflow==2.2.2->-r required_packages.txt (line 1)) (4.12.2)\n",
      "Collecting pyjwt>=1.7.0 (from databricks-cli<1,>=0.8.7->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading PyJWT-2.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting oauthlib>=3.1.0 (from databricks-cli<1,>=0.8.7->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.7 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.2.2->-r required_packages.txt (line 1)) (2.2.3)\n",
      "Collecting websocket-client>=0.32.0 (from docker<7,>=4.0.0->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from docker<7,>=4.0.0->mlflow==2.2.2->-r required_packages.txt (line 1)) (308)\n",
      "Collecting dictdiffer>=0.8.1 (from dvc-data<3.17,>=3.16.2->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached dictdiffer-0.9.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting diskcache>=5.2.1 (from dvc-data<3.17,>=3.16.2->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sqltrie<1,>=0.11.0 (from dvc-data<3.17,>=3.16.2->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached sqltrie-0.11.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting orjson<4,>=3 (from dvc-data<3.17,>=3.16.2->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached orjson-3.10.11-cp310-none-win_amd64.whl.metadata (52 kB)\n",
      "Collecting aiohttp-retry>=2.5.0 (from dvc-http>=2.29.0->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting billiard<5.0,>=4.2.0 (from celery->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached billiard-4.2.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting vine<6.0,>=5.1.0 (from celery->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached vine-5.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting click-didyoumean>=0.3.0 (from celery->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached click_didyoumean-0.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting click-repl>=0.2.0 (from celery->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached click_repl-0.3.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting click-plugins>=1.1.1 (from celery->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting atpublic (from flufl.lock<9,>=8.1.0->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached atpublic-5.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=2.1.0->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting typer>=0.4.1 (from gto<2,>=1.6.0->dvc->-r required_packages.txt (line 10))\n",
      "  Downloading typer-0.13.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic!=2.0.0,<3,>=1.9.0 (from gto<2,>=1.6.0->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting semver>=2.13.0 (from gto<2,>=1.6.0->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting appdirs (from iterative-telemetry>=0.0.7->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow==2.2.2->-r required_packages.txt (line 1)) (3.0.2)\n",
      "Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9)) (5.7.2)\n",
      "Collecting jupyter-events>=0.9.0 (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading jupyter_events-0.10.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting nbconvert>=6.4.4 (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading nbconvert-7.16.4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading prometheus_client-0.21.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pywinpty>=2.0.1 (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading pywinpty-2.0.14-cp310-none-win_amd64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9)) (26.2.0)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9)) (5.14.3)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting httpx>=0.25.0 (from jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (6.29.5)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (65.5.0)\n",
      "Collecting tomli>=1.2.2 (from jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading tomli-2.1.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading json5-0.9.28-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.27.1->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting amqp<6.0.0,>=5.1.1 (from kombu->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached amqp-5.3.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow==2.2.2->-r required_packages.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow==2.2.2->-r required_packages.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow==2.2.2->-r required_packages.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from rich>=12->dvc->-r required_packages.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from rich>=12->dvc->-r required_packages.txt (line 10)) (2.18.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.11->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached ruamel.yaml.clib-0.2.12-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn<2->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pygit2>=1.14.0 (from scmrepo<4,>=3.3.8->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached pygit2-1.16.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting asyncssh<3,>=2.13.1 (from scmrepo<4,>=3.3.8->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached asyncssh-2.18.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting slicer==0.0.8 (from shap<1,>=0.40->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numba (from shap<1,>=0.40->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached numba-0.60.0-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy<3,>=1.4.0->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown->-r required_packages.txt (line 8))\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown->-r required_packages.txt (line 8))\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohttp (from aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc->-r required_packages.txt (line 10))\n",
      "  Downloading aiohttp-3.11.2-cp310-cp310-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting sniffio>=1.1 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9)) (1.2.2)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting cryptography>=39.0 (from asyncssh<3,>=2.13.1->scmrepo<4,>=3.3.8->dvc->-r required_packages.txt (line 10))\n",
      "  Downloading cryptography-43.0.3-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.36 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from click-repl>=0.2.0->celery->dvc->-r required_packages.txt (line 10)) (3.0.48)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (1.8.8)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (8.29.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (1.6.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r required_packages.txt (line 9))\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading rpds_py-0.21.0-cp310-none-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=12->dvc->-r required_packages.txt (line 10)) (0.1.2)\n",
      "Collecting bleach!=5.0.0 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading mistune-3.0.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading nbclient-0.10.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting tinycss2 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading fastjsonschema-2.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=2.0.0,<3,>=1.9.0->gto<2,>=1.6.0->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic!=2.0.0,<3,>=1.9.0->gto<2,>=1.6.0->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached pydantic_core-2.23.4-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting cffi>=1.17.0 (from pygit2>=1.14.0->scmrepo<4,>=3.3.8->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached cffi-1.17.1-cp310-cp310-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.4.1->gto<2,>=1.6.0->dvc->-r required_packages.txt (line 10))\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->shap<1,>=0.40->mlflow==2.2.2->-r required_packages.txt (line 1))\n",
      "  Using cached llvmlite-0.43.0-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc->-r required_packages.txt (line 10))\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc->-r required_packages.txt (line 10))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc->-r required_packages.txt (line 10))\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc->-r required_packages.txt (line 10))\n",
      "  Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc->-r required_packages.txt (line 10))\n",
      "  Downloading propcache-0.2.0-cp310-cp310-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc->-r required_packages.txt (line 10))\n",
      "  Downloading yarl-1.17.2-cp310-cp310-win_amd64.whl.metadata (68 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->aiohttp-retry>=2.5.0->dvc-http>=2.29.0->dvc->-r required_packages.txt (line 10))\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pycparser (from cffi>=1.17.0->pygit2>=1.14.0->scmrepo<4,>=3.3.8->dvc->-r required_packages.txt (line 10))\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: decorator in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (0.19.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (0.6.3)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Using cached webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from prompt-toolkit>=3.0.36->click-repl>=0.2.0->celery->dvc->-r required_packages.txt (line 10)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (0.8.4)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\omar hady\\data projects\\machine learning\\end-to-end cancer evaluation\\end-to-end-cancer-classification-using-mlflow\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook->-r required_packages.txt (line 9)) (0.2.3)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->-r required_packages.txt (line 9))\n",
      "  Using cached types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n",
      "Using cached mlflow-2.2.2-py3-none-any.whl (17.6 MB)\n",
      "Using cached ensure-1.0.2-py2.py3-none-any.whl (15 kB)\n",
      "Using cached python_box-6.0.2-cp310-cp310-win_amd64.whl (942 kB)\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Using cached types_PyYAML-6.0.12.20240917-py3-none-any.whl (15 kB)\n",
      "Downloading scipy-1.14.1-cp310-cp310-win_amd64.whl (44.8 MB)\n",
      "   ---------------------------------------- 0.0/44.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/44.8 MB 5.6 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 2.1/44.8 MB 5.6 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 4.2/44.8 MB 7.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 6.8/44.8 MB 8.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 9.2/44.8 MB 9.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 11.8/44.8 MB 9.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 14.2/44.8 MB 10.1 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 16.8/44.8 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 19.4/44.8 MB 10.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 22.0/44.8 MB 10.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 24.4/44.8 MB 10.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 26.7/44.8 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 28.8/44.8 MB 10.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 31.2/44.8 MB 10.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 33.6/44.8 MB 10.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 36.2/44.8 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 38.8/44.8 MB 10.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.1/44.8 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.4/44.8 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.8/44.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.8/44.8 MB 10.2 MB/s eta 0:00:00\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.4/11.6 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.7/11.6 MB 13.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.3/11.6 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.0/11.6 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 12.1 MB/s eta 0:00:00\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading notebook-7.2.2-py3-none-any.whl (5.0 MB)\n",
      "   ---------------------------------------- 0.0/5.0 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 2.1/5.0 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.0/5.0 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.0/5.0 MB 11.8 MB/s eta 0:00:00\n",
      "Using cached dvc-3.56.0-py3-none-any.whl (456 kB)\n",
      "Using cached tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Downloading matplotlib-3.9.2-cp310-cp310-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.4/7.8 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.7/7.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 11.5 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached flask-2.3.3-py3-none-any.whl (96 kB)\n",
      "Using cached Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
      "Using cached alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Downloading contourpy-1.3.1-cp310-cp310-win_amd64.whl (218 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "Using cached dpath-2.2.0-py3-none-any.whl (17 kB)\n",
      "Using cached dvc_data-3.16.7-py3-none-any.whl (78 kB)\n",
      "Using cached dvc_http-2.32.0-py3-none-any.whl (12 kB)\n",
      "Using cached dvc_objects-5.1.0-py3-none-any.whl (33 kB)\n",
      "Using cached dvc_render-1.0.2-py3-none-any.whl (22 kB)\n",
      "Using cached dvc_studio_client-0.21.0-py3-none-any.whl (16 kB)\n",
      "Using cached dvc_task-0.40.2-py3-none-any.whl (21 kB)\n",
      "Using cached celery-5.4.0-py3-none-any.whl (425 kB)\n",
      "Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Using cached flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
      "Using cached flufl_lock-8.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading fonttools-4.55.0-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 12.5 MB/s eta 0:00:00\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Using cached grandalf-0.8-py3-none-any.whl (41 kB)\n",
      "Using cached gto-1.7.1-py3-none-any.whl (46 kB)\n",
      "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Using cached importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Using cached iterative_telemetry-0.0.9-py3-none-any.whl (10 kB)\n",
      "Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading jupyter_server-2.14.2-py3-none-any.whl (383 kB)\n",
      "Downloading jupyterlab-4.2.6-py3-none-any.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/11.6 MB 12.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.5/11.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.8/11.6 MB 11.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.4/11.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Downloading kiwisolver-1.4.7-cp310-cp310-win_amd64.whl (55 kB)\n",
      "Using cached kombu-5.4.2-py3-none-any.whl (201 kB)\n",
      "Using cached vine-5.1.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading pillow-11.0.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 2.4/2.6 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 12.3 MB/s eta 0:00:00\n",
      "Using cached platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
      "Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached pyarrow-11.0.0-cp310-cp310-win_amd64.whl (20.6 MB)\n",
      "Using cached pydot-3.0.2-py3-none-any.whl (35 kB)\n",
      "Using cached pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Using cached pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Using cached ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
      "Downloading scikit_learn-1.5.2-cp310-cp310-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.1/11.0 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.1/11.0 MB 12.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 12.1 MB/s eta 0:00:00\n",
      "Using cached scmrepo-3.3.8-py3-none-any.whl (73 kB)\n",
      "Downloading dulwich-0.22.6-cp310-cp310-win_amd64.whl (600 kB)\n",
      "   ---------------------------------------- 0.0/600.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 600.5/600.5 kB 10.9 MB/s eta 0:00:00\n",
      "Using cached shap-0.46.0-cp310-cp310-win_amd64.whl (456 kB)\n",
      "Using cached slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Using cached shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
      "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading SQLAlchemy-2.0.36-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 14.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 10.6 MB/s eta 0:00:00\n",
      "Downloading sqlparse-0.5.2-py3-none-any.whl (44 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached voluptuous-0.15.2-py3-none-any.whl (31 kB)\n",
      "Using cached waitress-2.1.2-py3-none-any.whl (57 kB)\n",
      "Using cached zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
      "Using cached amqp-5.3.1-py3-none-any.whl (50 kB)\n",
      "Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Downloading argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
      "Using cached asyncssh-2.18.0-py3-none-any.whl (367 kB)\n",
      "Downloading babel-2.16.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.4/9.6 MB 12.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.7/9.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.8/9.6 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.4/9.6 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 10.9 MB/s eta 0:00:00\n",
      "Using cached billiard-4.2.1-py3-none-any.whl (86 kB)\n",
      "Using cached click_didyoumean-0.3.1-py3-none-any.whl (3.6 kB)\n",
      "Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Using cached click_repl-0.3.0-py3-none-any.whl (10 kB)\n",
      "Using cached dictdiffer-0.9.0-py2.py3-none-any.whl (16 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading greenlet-3.1.1-cp310-cp310-win_amd64.whl (298 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading json5-0.9.28-py3-none-any.whl (30 kB)\n",
      "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading jupyter_events-0.10.0-py3-none-any.whl (18 kB)\n",
      "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
      "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading nbconvert-7.16.4-py3-none-any.whl (257 kB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached orjson-3.10.11-cp310-none-win_amd64.whl (136 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading prometheus_client-0.21.0-py3-none-any.whl (54 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp310-none-win_amd64.whl (1.9 MB)\n",
      "Using cached pygit2-1.16.0-cp310-cp310-win_amd64.whl (1.3 MB)\n",
      "Downloading PyJWT-2.10.0-py3-none-any.whl (23 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading pywinpty-2.0.14-cp310-none-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 10.3 MB/s eta 0:00:00\n",
      "Using cached ruamel.yaml.clib-0.2.12-cp310-cp310-win_amd64.whl (118 kB)\n",
      "Using cached semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached sqltrie-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tomli-2.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading typer-0.13.1-py3-none-any.whl (44 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached atpublic-5.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "Using cached numba-0.60.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "Downloading aiohttp-3.11.2-cp310-cp310-win_amd64.whl (440 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Using cached cffi-1.17.1-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Downloading cryptography-43.0.3-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 2.1/3.1 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.1/3.1 MB 11.2 MB/s eta 0:00:00\n",
      "Downloading fastjsonschema-2.20.0-py3-none-any.whl (23 kB)\n",
      "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached llvmlite-0.43.0-cp310-cp310-win_amd64.whl (28.1 MB)\n",
      "Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
      "Downloading nbclient-0.10.0-py3-none-any.whl (25 kB)\n",
      "Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading rpds_py-0.21.0-cp310-none-win_amd64.whl (218 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)\n",
      "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.2.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "Using cached webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading yarl-1.17.2-cp310-cp310-win_amd64.whl (89 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n",
      "Installing collected packages: webencodings, pytz, pygtrie, funcy, fastjsonschema, dictdiffer, appdirs, antlr4-python3-runtime, zipp, zc.lockfile, websocket-client, webcolors, waitress, voluptuous, vine, uri-template, tzdata, types-pyYAML, types-python-dateutil, tqdm, tomlkit, tomli, tinycss2, threadpoolctl, tabulate, sqlparse, soupsieve, sniffio, smmap, slicer, shtab, shortuuid, shellingham, send2trash, semver, ruamel.yaml.clib, rpds-py, rfc3986-validator, rfc3339-validator, querystring-parser, pyYAML, pywinpty, python-json-logger, python-box, PySocks, pyparsing, pyjwt, pydantic-core, pycparser, protobuf, propcache, prometheus-client, platformdirs, pillow, pathspec, pandocfilters, packaging, overrides, orjson, oauthlib, numpy, networkx, multidict, mistune, Mako, llvmlite, kiwisolver, jupyterlab-pygments, jsonpointer, json5, joblib, Jinja2, itsdangerous, h11, greenlet, fsspec, frozenlist, fqdn, fonttools, flatten-dict, filelock, entrypoints, ensure, dvc-render, dulwich, dpath, distro, diskcache, defusedxml, cycler, configobj, cloudpickle, click, blinker, bleach, billiard, babel, attrs, atpublic, async-timeout, async-lru, annotated-types, aiohappyeyeballs, yarl, terminado, sqltrie, sqlalchemy, scipy, ruamel.yaml, referencing, pydot, pydantic, pyarrow, pandas, omegaconf, numba, iterative-telemetry, importlib-metadata, httpcore, grandalf, gitdb, flufl.lock, Flask, dvc-studio-client, dvc-objects, docker, databricks-cli, contourpy, click-repl, click-plugins, click-didyoumean, cffi, beautifulsoup4, arrow, anyio, amqp, aiosignal, typer, scikit-learn, pygit2, matplotlib, kombu, jupyter-server-terminals, jsonschema-specifications, isoduration, hydra-core, httpx, gitpython, gdown, Flask-cors, dvc-data, cryptography, argon2-cffi-bindings, alembic, aiohttp, shap, seaborn, jsonschema, celery, asyncssh, argon2-cffi, aiohttp-retry, scmrepo, nbformat, mlflow, dvc-task, dvc-http, nbclient, jupyter-events, gto, nbconvert, dvc, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 4.3.6\n",
      "    Uninstalling platformdirs-4.3.6:\n",
      "      Successfully uninstalled platformdirs-4.3.6\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed Flask-2.3.3 Flask-cors-5.0.0 Jinja2-3.1.4 Mako-1.3.6 PySocks-1.7.1 aiohappyeyeballs-2.4.3 aiohttp-3.11.2 aiohttp-retry-2.9.1 aiosignal-1.3.1 alembic-1.14.0 amqp-5.3.1 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 anyio-4.6.2.post1 appdirs-1.4.4 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 async-lru-2.0.4 async-timeout-5.0.1 asyncssh-2.18.0 atpublic-5.0 attrs-24.2.0 babel-2.16.0 beautifulsoup4-4.12.3 billiard-4.2.1 bleach-6.2.0 blinker-1.9.0 celery-5.4.0 cffi-1.17.1 click-8.1.7 click-didyoumean-0.3.1 click-plugins-1.1.1 click-repl-0.3.0 cloudpickle-2.2.1 configobj-5.0.9 contourpy-1.3.1 cryptography-43.0.3 cycler-0.12.1 databricks-cli-0.18.0 defusedxml-0.7.1 dictdiffer-0.9.0 diskcache-5.6.3 distro-1.9.0 docker-6.1.3 dpath-2.2.0 dulwich-0.22.6 dvc-3.56.0 dvc-data-3.16.7 dvc-http-2.32.0 dvc-objects-5.1.0 dvc-render-1.0.2 dvc-studio-client-0.21.0 dvc-task-0.40.2 ensure-1.0.2 entrypoints-0.4 fastjsonschema-2.20.0 filelock-3.16.1 flatten-dict-0.4.2 flufl.lock-8.1.0 fonttools-4.55.0 fqdn-1.5.1 frozenlist-1.5.0 fsspec-2024.10.0 funcy-2.0 gdown-5.2.0 gitdb-4.0.11 gitpython-3.1.43 grandalf-0.8 greenlet-3.1.1 gto-1.7.1 h11-0.14.0 httpcore-1.0.7 httpx-0.27.2 hydra-core-1.3.2 importlib-metadata-6.11.0 isoduration-20.11.0 iterative-telemetry-0.0.9 itsdangerous-2.2.0 joblib-1.4.2 json5-0.9.28 jsonpointer-3.0.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 jupyter-events-0.10.0 jupyter-lsp-2.2.5 jupyter-server-2.14.2 jupyter-server-terminals-0.5.3 jupyterlab-4.2.6 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 kiwisolver-1.4.7 kombu-5.4.2 llvmlite-0.43.0 matplotlib-3.9.2 mistune-3.0.2 mlflow-2.2.2 multidict-6.1.0 nbclient-0.10.0 nbconvert-7.16.4 nbformat-5.10.4 networkx-3.4.2 notebook-7.2.2 notebook-shim-0.2.4 numba-0.60.0 numpy-1.26.4 oauthlib-3.2.2 omegaconf-2.3.0 orjson-3.10.11 overrides-7.7.0 packaging-23.2 pandas-2.2.3 pandocfilters-1.5.1 pathspec-0.12.1 pillow-11.0.0 platformdirs-3.11.0 prometheus-client-0.21.0 propcache-0.2.0 protobuf-4.25.5 pyYAML-6.0.2 pyarrow-11.0.0 pycparser-2.22 pydantic-2.9.2 pydantic-core-2.23.4 pydot-3.0.2 pygit2-1.16.0 pygtrie-2.5.0 pyjwt-2.10.0 pyparsing-3.2.0 python-box-6.0.2 python-json-logger-2.0.7 pytz-2022.7.1 pywinpty-2.0.14 querystring-parser-1.2.4 referencing-0.35.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.21.0 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.12 scikit-learn-1.5.2 scipy-1.14.1 scmrepo-3.3.8 seaborn-0.13.2 semver-3.0.2 send2trash-1.8.3 shap-0.46.0 shellingham-1.5.4 shortuuid-1.0.13 shtab-1.7.1 slicer-0.0.8 smmap-5.0.1 sniffio-1.3.1 soupsieve-2.6 sqlalchemy-2.0.36 sqlparse-0.5.2 sqltrie-0.11.1 tabulate-0.9.0 terminado-0.18.1 threadpoolctl-3.5.0 tinycss2-1.4.0 tomli-2.1.0 tomlkit-0.13.2 tqdm-4.67.0 typer-0.13.1 types-pyYAML-6.0.12.20240917 types-python-dateutil-2.9.0.20241003 tzdata-2024.2 uri-template-1.3.0 vine-5.1.0 voluptuous-0.15.2 waitress-2.1.2 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0 yarl-1.17.2 zc.lockfile-3.0.post1 zipp-3.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r required_packages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Omar Hady\\\\Data Projects\\\\Machine Learning\\\\End-To-End Cancer Evaluation\\\\End-To-End-Cancer-Classification-Using-MLflow'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data class for base model (update entity)\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class BaseModelConfig:\n",
    "    root_dir: Path\n",
    "    base_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    params_image_size: list\n",
    "    params_learning_rate: float\n",
    "    params_include_top: bool\n",
    "    params_weight: str\n",
    "    params_classes: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a method in the configuration management file (uodate configuration manager)\n",
    "\n",
    "from src.Chest_Cancer_Classification.constants import *\n",
    "from src.Chest_Cancer_Classification.utils.common import create_directories, read_yaml\n",
    "from src.Chest_Cancer_Classification.entity.config_entity import DataIngestionConfiguration\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    '''\n",
    "    General Configuration class containing configuration methods for every component\n",
    "    '''\n",
    "    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    \n",
    "    def get_base_model(self)-> BaseModelConfig:\n",
    "        config = self.config.prepare_base_model\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        base_model_config = BaseModelConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            base_model_path = config.base_model_path,\n",
    "            updated_base_model_path = config.updated_base_model_path,\n",
    "            params_image_size = self.params.IMAGE_SIZE,\n",
    "            params_learning_rate= self.params.LEARNING_RATE,\n",
    "            params_include_top= self.params.INCLUDE_TOP,\n",
    "            params_weight= self.params.WEIGHTS,\n",
    "            params_classes= self.params.CLASSES)\n",
    "        return base_model_config\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the component \n",
    "\n",
    "class BaseModel:\n",
    "    def __init__(self, config: BaseModelConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def create_base_model(self):\n",
    "        self.model = tf.keras.applications.vgg16.VGG16(\n",
    "            input_shape = self.config.params_image_size,\n",
    "            weights = self.config.params_weight,\n",
    "            include_top = self.config.params_include_top\n",
    "\n",
    "        )\n",
    "\n",
    "        self.save_model(path= self.config.base_model_path, model=self.model)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path:Path, model: tf.keras.Model):\n",
    "        return model.save(path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _prepare_full_model(model, classes, freeze_all, freeze_till, learning_rate):\n",
    "        ''' \n",
    "        A static method to create the full model using a base model from transfer learning\n",
    "\n",
    "        Arguments:\n",
    "            model (keras.Model object): base model\n",
    "            classes: no. of classes that the model will classify\n",
    "            freeze_all (bool): if True freezes all the layers of the base model without updating\n",
    "            freeze_till (int) : if given, the number of layers that shouldn't be updated \n",
    "                                and the layers after it will be updated\n",
    "            learning_rate (float): model learning rate for updating\n",
    "        '''\n",
    "\n",
    "        if freeze_all:\n",
    "            for layers in model.layers:\n",
    "                model.trainable = False\n",
    "        elif (freeze_till is not None):\n",
    "            for layer in model.layers[:-freeze_till]:\n",
    "                model.trainable = False\n",
    "\n",
    "        flatten_in = tf.keras.layers.Flatten()(model.output)\n",
    "        predict_in = tf.keras.layers.Dense(units=classes, activation = \"softmax\")(flatten_in)\n",
    "\n",
    "        full_model = tf.keras.models.Model(inputs=model.input, outputs=predict_in )\n",
    "\n",
    "        full_model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        full_model.summary()\n",
    "        return full_model\n",
    "    \n",
    "    def update_base_model(self):\n",
    "        self.full_model = self._prepare_full_model(\n",
    "            model=self.model,\n",
    "            classes=self.config.params_classes,\n",
    "            freeze_all=True,\n",
    "            freeze_till=None,\n",
    "            learning_rate=self.config.params_learning_rate\n",
    "        )\n",
    "\n",
    "        self.save_model(path=self.config.updated_base_model_path, model=self.full_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-19 02:06:00,492 - INFO - common - config\\config.yaml loaded successfully]\n",
      "[2024-11-19 02:06:00,495 - INFO - common - params.yaml loaded successfully]\n",
      "[2024-11-19 02:06:00,495 - INFO - common - created directory at: artifacts]\n",
      "[2024-11-19 02:06:00,496 - INFO - common - created directory at: artifacts/prepare_base_model]\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n",
      "[2024-11-19 02:06:08,255 - WARNING - saving_api - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> \n",
       "\n",
       " block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> \n",
       "\n",
       " block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> \n",
       "\n",
       " block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> \n",
       "\n",
       " block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> \n",
       "\n",
       " block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> \n",
       "\n",
       " block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> \n",
       "\n",
       " block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> \n",
       "\n",
       " block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> \n",
       "\n",
       " block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> \n",
       "\n",
       " block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> \n",
       "\n",
       " block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> \n",
       "\n",
       " block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> \n",
       "\n",
       " block5_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">50,178</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m1,792\u001b[0m \n",
       "\n",
       " block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)           \u001b[38;5;34m36,928\u001b[0m \n",
       "\n",
       " block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)                \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)          \u001b[38;5;34m73,856\u001b[0m \n",
       "\n",
       " block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)         \u001b[38;5;34m147,584\u001b[0m \n",
       "\n",
       " block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)           \u001b[38;5;34m295,168\u001b[0m \n",
       "\n",
       " block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)           \u001b[38;5;34m590,080\u001b[0m \n",
       "\n",
       " block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)           \u001b[38;5;34m590,080\u001b[0m \n",
       "\n",
       " block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m1,180,160\u001b[0m \n",
       "\n",
       " block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m2,359,808\u001b[0m \n",
       "\n",
       " block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m2,359,808\u001b[0m \n",
       "\n",
       " block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m2,359,808\u001b[0m \n",
       "\n",
       " block5_conv2 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m2,359,808\u001b[0m \n",
       "\n",
       " block5_conv3 (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m2,359,808\u001b[0m \n",
       "\n",
       " block5_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " flatten (\u001b[38;5;33mFlatten\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)                       \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                      \u001b[38;5;34m50,178\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,764,866</span> (56.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,764,866\u001b[0m (56.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,178</span> (196.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,178\u001b[0m (196.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-19 02:06:08,372 - WARNING - saving_api - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    }
   ],
   "source": [
    "#pipeline\n",
    "\n",
    "config = ConfigurationManager()\n",
    "prepare_model_config = config.get_base_model()\n",
    "base_model = BaseModel(config=prepare_model_config)\n",
    "base_model.create_base_model()\n",
    "base_model.update_base_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
